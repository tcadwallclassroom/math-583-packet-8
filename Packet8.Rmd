---
title: "Packet 8 - The Chi-squared statistic"
knit: (function(input_file, encoding) {
  out_dir <- 'docs';
  rmarkdown::render(input_file,
  encoding=encoding,
  output_file=file.path(dirname(input_file), out_dir, 'index.html'))})
author: "Todd CadwalladerOlsker"
date: "*Last updated:* `r Sys.Date()`"
output:
  rmdformats::downcute:
    downcute_theme: "chaos"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(rmdformats)
library(openintro)
library(tidyverse)
library(gghighlight)
library(formatR)
library(infer)
knitr::opts_chunk$set(echo = T, 
                      cache = T, 
                      eval = T, 
                      cache.lazy = F, 
                      warning = F)
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=F)
options(scipen = 999)
```




## The $\chi^2$ statistic

Shifting our perspective a little bit from the previous packet: We have been examining whether or not the variable of "Was there a hospitalization event?" is *independent* of the variable of "Was the patient treated with ivermectin or a placebo?". In the null hypothesis, these variables are, in fact, independent: the probability of having an event is the same for those treated with ivermectin as those treated with a placebo. 

We can create *two way tables* from our tables above, in which we find both the sums of the rows and the sums of the columns. The magic of a two way table is that we can have more than just a 2-by-2 table; we can make tables of any size. 

As an example, let's look at whether the population favors requiring a permit to carry a gun, broken down by race:

```{r gunlaw race}
library(gssr)
num_vars <- c()
cat_vars <- c("race", "gunlaw")
my_vars <- c(num_vars, cat_vars)

gss18 <- gss_get_yr(2018)
data <- gss18
data <- data %>% 
  select(all_of(my_vars)) %>% 
  mutate(
    across(everything(), haven::zap_missing),
    across(all_of(cat_vars), forcats::as_factor)
  )

table_data <- table(data$race, data$gunlaw)
table_data %>% addmargins() 
table_data %>% prop.table(1)
table_m <- table_data %>% addmargins() 
```

We can see that white respondents are slightly less that 70% in support of gun permit laws, while black and other respondents are slightly more than 75% in favor. 

Is this difference statistically significant? In order to answer that question, we first need a test statistic. 

The agreed-upon statistic for a two-way table like this is the *chi-square* or $\chi^2$ statistic. It is defined as

\[\chi^2 = \sum \frac{(O-E)^2}{E}\]

where $O$ is the observed value of each cell, $E$ is the expected value of each cell, and the summation is over all cells of the table. The expected value, $E$, is given by the null hypothesis: normally, that there is no difference in the proportion of favoring gun permit laws among the groups. Under this null hypothesis, the expected value is the product of the row sum and the column sum, divided by the total sum:

```{r calc chisq by hand}
E_white_favor <- 
  table_m["white", "Sum"]*table_m["Sum", "favor"]/table_m["Sum", "Sum"]
E_black_favor <- 
  table_m["black", "Sum"]*table_m["Sum", "favor"]/table_m["Sum", "Sum"]
E_other_favor <- 
  table_m["other", "Sum"]*table_m["Sum", "favor"]/table_m["Sum", "Sum"]
E_white_oppose <- 
  table_m["white", "Sum"]*table_m["Sum", "oppose"]/table_m["Sum", "Sum"]
E_black_oppose <- 
  table_m["black", "Sum"]*table_m["Sum", "oppose"]/table_m["Sum", "Sum"]
E_other_oppose <- 
  table_m["other", "Sum"]*table_m["Sum", "oppose"]/table_m["Sum", "Sum"]

chi_sq <- 
  (table_data["white", "favor"] - E_white_favor)^2 / E_white_favor +
  (table_data["black", "favor"] - E_black_favor)^2 / E_black_favor +
  (table_data["other", "favor"] - E_other_favor)^2 / E_other_favor +
  (table_data["white", "oppose"] - E_white_oppose)^2 / E_white_oppose +
  (table_data["black", "oppose"] - E_black_oppose)^2 / E_black_oppose +
  (table_data["other", "oppose"] - E_other_oppose)^2 / E_other_oppose

chi_sq
```

Obviously, calculating this by hand is a pain. The `infer` package can help:

```{r chisq infer}
observed_chisq <- data %>%
  specify(gunlaw ~ race) %>%
  calculate(stat = "Chisq")
observed_chisq
```

What does this statistic tell us? Is `r chi_sq` high? Low?

Using randomization techniques, we can find out what proportion of simulations have a more extreme (that is, larger) $\chi^2$ statistic than our observed data.

```{r chisq infer rando}
null_dist_chisq <- data %>%
  specify(gunlaw ~ race) %>%
  hypothesize(null = "independence") %>% 
  generate(reps = 10000, type = "permute") %>% 
  calculate(stat = "Chisq")
  
null_dist_chisq %>%
  visualize(bins = 100) + 
  shade_p_value(observed_chisq,
                direction = "greater")

null_dist_chisq %>% 
  get_p_value(observed_chisq, direction = "greater")
```

The distribution you see in the graph is the $\chi^2$ distribution with 2 "degrees of freedom". We can graph the probability distribution function along with our data:

```{r chisq viz}
null_dist_chisq %>%
  visualize(bins = 100, method = "both") + 
  shade_p_value(observed_chisq,
                direction = "greater")
```

Of course, R also has a built-in function `chisq.test()`:

```{r chisq baseR}
chisq.test(data$race, data$gunlaw)
```

The 2-by-2 tables we saw in the two-proportion tests above can also be tackled using the $\chi^2$ statistic instead. In this case, there is only 1 degree of freedom. 

The number of degrees of freedom is 
\[(\text{number of rows} - 1)(\text{number of columns} - 1)\] when there are at least two rows and two columns.

The idea here is that if we know this many entries in the table, we can calculate the rest of them using the (known) values of the marginal sums. 

## Goodness-of-fit

Another use of the $\chi^2$ statistic is to perform what is called a *goodness-of-fit test*. Rather than determine if two variables are independent, this time we will investigate whether observed data fits a known distribution.

For this test, let's examine the data of fatal police shootings compiled by the Washington Post. The data is available as a .csv file at [Washinton Post's Github repository.](https://github.com/washingtonpost/data-police-shootings) Click on the green "code" button, then "Download ZIP". Then, extract the zip file, and load the .csv file into R. 

```{r load csv}
fatal_police_shootings_data <- 
  read_csv("data-police-shootings-master/fatal-police-shootings-data.csv")
```

Before we do anything sophisticated with this data, let's play around with it. As we do, I want you to ask you:

* What do you *notice*?
* What do you *wonder*?

1. Start by creating several tables for categorical variables. Gender and race are big ones, but examine the data for other categorical variables as well. Try turning some of these tables into bar charts using `ggplot`.
2. Use the `summary` command to find the mean and five-number summary for some numeric variables. Create some histograms for numeric variables. 

Now, let's think about what we can infer from our data, specifically about race. For convenience, let's recode our data:

```{r recode}
fps_data <- fatal_police_shootings_data %>% 
  mutate(
    race_full = recode(race,
                     "A" = "Asian",
                     "B" = "Black",
                     "H" = "Hispanic",
                     "N" = "Native American",
                     "O" = "Other",
                     "W" = "White"
    )
  )
```

Here is some R code that stores some demographic information for us; this information is taken from the 2020 census:

```{r demographics}
demographics_2020 <- c("Asian" = 0.059,
                       "Black" = 0.119,
                       "Hispanic" = 0.195,
                       "Native American" = 0.009,
                       "Other" = 0.045,
                       "White" = 0.573)
```

The population of  the US was approximately 331.5 million in 2020, we can find the number of Americans in each group (in "hundred thousands") with:

```{r demo times number}
demographics_2020 * 3315
```

and then, find the number of police shootings (per 100,000) for each group with

```{r per capita}
table(fps_data$race_full) / (demographics_2020 * 3315)
```

Now, the disparities here are pretty obvious, but let's put some inferential statistics on it. 

3. If we want to think about the question, "Are certain race groups fatally shot by police at a higher rate than others?", what would your null and alternative hypotheses be?

4. Based on the null hypothesis, what would be the "expected" number of fatal police shootings for each group? (Use the total number of fatal police shootings to calculate the expected values)

5. Use these expected values and the observed data to calculate the $\chi^2$ statistic for this data. Do this by hand first, then verify your answer with the `infer` package. See the [infer package documentation](https://cran.microsoft.com/snapshot/2022-03-29/web/packages/infer/vignettes/chi_squared.html) on the chi-squared tests; the "goodness of fit" test is about halfway down.

6. Use the `infer` package to generate 10000 simulations of data drawn from the null hypothesis.

7. Finally, use `infer` to calculate the p-value, and use the `visualize()` function of `infer` to create a graph of the simulated null distribution with the observed data highlighted.

8. Now, run a `chisq.test()` on the data, with `p = demographics_2020`. (See the help file for more information.)

## To Turn In

Turn in a report of the results of steps 3-7 above. You should use the same steps, adapted as necessary, as you did in your report from Packet 7. I've included those instructions below along with the necessary adaptations. Note that you will not need to calculate a confidence interval (which would be kind of meaningless for the $\chi^2$ statistic, anyways.)

Section 1: Introduction
(This section covers Step 1: Ask a research question.)

Provide some background context for the investigation. What is the general topic we are investigating, and why is it interesting or important?
What is the research question in this investigation?

Section 2: Methods
(This section covers Step 2: Design a study and collect data.)

What is the population we are studying? What does the goodness of fit test compare our data to?

Section 3: Data Exploration
(This section covers Step 3: Explore the data.)

In this section, you will provide summary tables and graphs to organize and visualize the data collected by the researchers. Include one bar chart showing the number of fatal police shootings for each race, and another showing the number of fatal police shootings *per capita* for each group.


Section 4: Statistical Inference
(This section covers Step 4: Draw inferences beyond the data.) 

Tests of Significance: Identify the two competing explanations and identify which is the null vs. alternative hypothesis. 

Perform the first S in the 3S strategy: What is the (test) statistic we want to calculate from each observed or simulated sample? What is its value in the observed sample?

Perform the second S in the 3S strategy: Explain how to create a null distribution model for the study assuming the null hypothesis is correct. 

Perform the third S step in the 3S strategy: explain how to obtain the p-value for this test, and identify its value as calculated by R.

How strong is your evidence against the null hypothesis and in favor of the alternative hypothesis? Include a graph showing the null distribution of the test statistic, the position of the observed test statistic, and highlighting the portion of the null distribution that is at least as extreme as the observed test statistic. Write 1-2 sentences to interpret the graph.

Finally, re-do your analysis with a built-in R function, and see if you get similar results.

Section 5: Conclusion and Discussion
(This section covers Step 5: Formulate conclusions and Step 6: Look back and ahead.)

Write a conclusion that answers your research question.
Justify your conclusion based on the results of your test of significance.
What are the real-world consequences of your results?

## Reading Assignment

Read "Mathematics in Service to Democracy", by Kira Hamman (posted to Canvas). You should write a response to this article and turn it in with your report above. As you respond, ask yourself: Are there any characteristics of the current mathematics education system or curriculum that you believe should be changed in order to better promote quantitative reasoning (and democracy)? If so, what are they and why? What challenges are there in making those changes? What might be some unintended consequences of making those changes? 



